{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Reshape, Multiply\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from board import Board, BOARD_SIZE, ORDER_PLAYERS\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "def get_legal_actions_mask(self):\n",
    "    mask = np.zeros((len(self.board.pieces), BOARD_SIZE * 2 - 1, BOARD_SIZE * 2 - 1, BOARD_SIZE * 2, BOARD_SIZE * 2), dtype=np.int8)\n",
    "    \n",
    "    for piece_index, piece in enumerate(self.board.pieces):\n",
    "        if piece.color == self.current_player.color and not piece.is_dead:\n",
    "            legal_moves = piece.all_possible_moves(self.board)\n",
    "            for new_q, new_r in legal_moves:\n",
    "                mask[piece_index, new_q + BOARD_SIZE - 1, new_r + BOARD_SIZE - 1, :, :] = 1\n",
    "                \n",
    "                # Si le mouvement implique de tuer une pièce, ajoutez les positions légales pour la pièce tuée\n",
    "                target_piece = self.board.get_piece_at(new_q, new_r)\n",
    "                if target_piece and target_piece.color != piece.color:\n",
    "                    unoccupied_cells = self.board.get_unoccupied_cells()\n",
    "                    for killed_q, killed_r in unoccupied_cells:\n",
    "                        mask[piece_index, new_q + BOARD_SIZE - 1, new_r + BOARD_SIZE - 1, killed_q + BOARD_SIZE, killed_r + BOARD_SIZE] = 1\n",
    "                else:\n",
    "                    # Si aucune pièce n'est tuée, permettez seulement l'action \"pas de déplacement de pièce tuée\"\n",
    "                    mask[piece_index, new_q + BOARD_SIZE - 1, new_r + BOARD_SIZE - 1, BOARD_SIZE - 1, BOARD_SIZE - 1] = 1\n",
    "    \n",
    "    return mask\n",
    "\n",
    "class DjambiEnv:\n",
    "    def __init__(self):\n",
    "        self.board = Board()\n",
    "        self.current_player_index = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = Board()\n",
    "        self.current_player_index = 0\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        state = np.zeros((BOARD_SIZE*2-1, BOARD_SIZE*2-1, 7), dtype=np.int8)\n",
    "        piece_types = ['militant', 'assassin', 'chief', 'diplomat', 'necromobile', 'reporter']\n",
    "        for piece in self.board.pieces:\n",
    "            x, y = piece.q + BOARD_SIZE - 1, piece.r + BOARD_SIZE - 1\n",
    "            color_index = ORDER_PLAYERS.index(piece.color)\n",
    "            piece_type_index = piece_types.index(piece.piece_class) + 1  # +1 pour réserver 0 aux cases vides\n",
    "            state[x, y, color_index] = piece_type_index\n",
    "            if piece.is_dead:\n",
    "                state[x, y, 6] = 1  # Marquer les pièces mortes dans le 7e canal\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        legal_actions_mask = self.get_legal_actions_mask()\n",
    "        if legal_actions_mask[tuple(action)] == 0:\n",
    "            # L'action est illégale, appliquez une pénalité et ne changez pas l'état du jeu\n",
    "            return self.get_state(), -10, False, {\"illegal_move\": True, \"undo\": True}\n",
    "\n",
    "        # Sauvegardez l'état actuel avant d'exécuter l'action\n",
    "        previous_state = self.board.copy()\n",
    "        previous_player_index = self.current_player_index\n",
    "        \n",
    "        # Exécuter l'action\n",
    "        piece, new_position, killed_piece_position = self.decode_action(action)\n",
    "        piece.move(new_position[0], new_position[1], self.board)\n",
    "        \n",
    "        # Exécuter l'action et retourner new_state, reward, done, info\n",
    "        new_state = self.get_state()\n",
    "        reward = self.calculate_reward()\n",
    "        done = len(self.board.players) == 1\n",
    "        # Ne changez pas le joueur actuel si l'action était illégale\n",
    "        if not legal_actions_mask[tuple(action)] == 0:\n",
    "            self.current_player_index = (self.current_player_index + 1) % len(self.board.players)\n",
    "        \n",
    "        info = {\n",
    "            \"illegal_move\": False,\n",
    "            \"undo\": False,\n",
    "            \"previous_state\": previous_state,\n",
    "            \"previous_player_index\": previous_player_index\n",
    "        }\n",
    "        return new_state, reward, done, info\n",
    "\n",
    "    # Ajoutez une nouvelle méthode pour annuler le dernier mouvement\n",
    "    def undo_last_move(self, previous_state, previous_player_index):\n",
    "        self.board = previous_state\n",
    "        self.current_player_index = previous_player_index\n",
    "\n",
    "    def decode_action(self, action):\n",
    "        piece_index, new_q, new_r, killed_piece_q, killed_piece_r = action\n",
    "        \n",
    "        piece = self.board.pieces[piece_index]\n",
    "        new_position = (new_q, new_r)\n",
    "        killed_piece_position = (killed_piece_q, killed_piece_r) if killed_piece_q != -1 and killed_piece_r != -1 else None\n",
    "        \n",
    "        return piece, new_position, killed_piece_position\n",
    "\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        # Calculez la récompense basée sur l'état du jeu\n",
    "        # Par exemple, +1 pour tuer une pièce ennemie, +10 pour tuer un chef, etc.\n",
    "        pass\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0   # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        state_input = Input(shape=self.state_size)\n",
    "        legal_actions_mask = Input(shape=self.action_size)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_shape=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        output = Dense(np.prod(self.action_size))(x)\n",
    "        output = Reshape(self.action_size)(output)\n",
    "        masked_output = Multiply()([output, legal_actions_mask])\n",
    "        \n",
    "        model = Model(inputs=[state_input, legal_actions_mask], outputs=masked_output)\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "        \n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        legal_actions_mask = self.env.get_legal_actions_mask()\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # Choisissez une action aléatoire parmi les actions légales\n",
    "            legal_actions = np.argwhere(legal_actions_mask.flatten() == 1)\n",
    "            return legal_actions[np.random.randint(len(legal_actions))].reshape(-1)\n",
    "        \n",
    "        act_values = self.model.predict(state)\n",
    "        act_values[legal_actions_mask == 0] = -np.inf  # Masquer les actions illégales\n",
    "        return np.unravel_index(np.argmax(act_values), act_values.shape)\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Entraînement\n",
    "env = DjambiEnv()\n",
    "state_size = (BOARD_SIZE*2-1, BOARD_SIZE*2-1, 7)\n",
    "\n",
    "# Définition de l'espace d'action\n",
    "action_space = spaces.MultiDiscrete([\n",
    "    len(env.board.pieces),  # Nombre de pièces\n",
    "    BOARD_SIZE * 2 - 1,     # Plage de q\n",
    "    BOARD_SIZE * 2 - 1,     # Plage de r\n",
    "    BOARD_SIZE * 2,         # Plage de q pour la pièce tuée (inclut -1 pour \"pas de pièce tuée\")\n",
    "    BOARD_SIZE * 2          # Plage de r pour la pièce tuée (inclut -1 pour \"pas de pièce tuée\")\n",
    "])\n",
    "\n",
    "agent = DQNAgent(state_size, action_space)\n",
    "\n",
    "episodes = 1000\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    for time in range(500):  # max 500 moves per game\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if info[\"illegal_move\"]:\n",
    "            # Si le mouvement était illégal, annulez-le et faites rejouer le même joueur\n",
    "            env.undo_last_move(info[\"previous_state\"], info[\"previous_player_index\"])\n",
    "            continue\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(f\"episode: {e}/{episodes}, score: {time}\")\n",
    "            break\n",
    "    if len(agent.memory) > 32:\n",
    "        agent.replay(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from env import DjambiEnv\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "env = DjambiEnv()\n",
    "state_size = env.observation_space.shape\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Définir le modèle du réseau neuronal\n",
    "def build_model(state_size, action_size):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=state_size))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "model = build_model(state_size, action_size)\n",
    "\n",
    "# Paramètres d'entraînement\n",
    "episodes = 1000\n",
    "batch_size = 64\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "memory = deque(maxlen=2000)\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.array(state)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            act_values = model.predict(state.reshape(1, *state.shape))\n",
    "            action = np.argmax(act_values[0])\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.array(next_state)\n",
    "        total_reward += reward\n",
    "\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode: {e+1}/{episodes}, Score: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "        if len(memory) > batch_size:\n",
    "            minibatch = random.sample(memory, batch_size)\n",
    "            for s, a, r, s_next, d in minibatch:\n",
    "                target = r\n",
    "                if not d:\n",
    "                    t = model.predict(s_next.reshape(1, *state_size))[0]\n",
    "                    target = r + gamma * np.amax(t)\n",
    "                target_f = model.predict(s.reshape(1, *state_size))\n",
    "                target_f[0][a] = target\n",
    "                model.fit(s.reshape(1, *state_size), target_f, epochs=1, verbose=0)\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "djambi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
